{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5iHhbYjxn9N"
   },
   "source": [
    "# 1. 선형 회귀 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nslr5Q7hX1Q8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjJnWfNRX9Zt",
    "outputId": "8d35dbbf-bf31-45ed-c488-be2db7f4a2c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1c9dbc4930>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKqpBOFR_YhF"
   },
   "source": [
    "실습을 위한 기본적인 셋팅이 끝났습니다. 이제 훈련 데이터인 x_train과 y_train을 선언합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "h1YtJLENYGUO"
   },
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeEHeOkg_aLu"
   },
   "source": [
    "x_train과 x_train의 크기(shape)를 출력해보겠습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ctuxU-qzYIpu",
    "outputId": "43c2ed9f-b364-4054-9561-24f4361fc1af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmSvtigK_cbM"
   },
   "source": [
    "x_train의 값이 출력되고, x_train의 크기가 (3 × 1)임을 알 수 있습니다.\n",
    "y_train과 y_train의 크기(shape)를 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNg7dcxUYMAR",
    "outputId": "6309d657-9560-4050-a145-3d44ab2a48fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.],\n",
      "        [4.],\n",
      "        [6.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocB6asUu_gl2"
   },
   "source": [
    "y_train의 값이 출력되고, y_train의 크기가 (3 × 1)임을 알 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiEUZ-tO_iL-"
   },
   "source": [
    "선형 회귀란 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일입니다.\n",
    "그리고 가장 잘 맞는 직선을 정의하는 것은 바로 W와 b입니다.\n",
    "결론적으로 선형 회귀의 목표는 가장 잘 맞는 직선을 정의하는 W와 b의 값을 찾는 것입니다.\n",
    "우선 가중치 W를 0으로 초기화하고, 이 값을 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iN2MsCOxYPFu",
    "outputId": "f556d871-a97c-4ec2-933b-27109fd4cff4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros(1, requires_grad=True)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFCuI4Ir_n30"
   },
   "source": [
    "가중치 W가 0으로 초기화되어있으므로 0이 출력된 것을 확인할 수 있습니다. 위에서 requires_grad=True가 인자로 주어진 것을 확인할 수 있습니다. 이는 이 변수는 학습을 통해 계속 값이 변경되는 변수임을 의미합니다.\n",
    "\n",
    "마찬가지로 편향\n",
    "b도 0으로 초기화하고, 학습을 통해 값이 변경되는 변수임을 명시합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NyzHVrRJYWY8",
    "outputId": "ef1bac03-b3f7-41d1-8ca6-9cd3bc8cbab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "b = torch.zeros(1, requires_grad=True)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8AIXThM_pqI"
   },
   "source": [
    "현재 가중치 W와 b둘 다 0이므로 현 직선의 방정식은 다음과 같습니다.\n",
    "$y = 0 × x + 0$\n",
    "지금 상태에선 x에 어떤 값이 들어가도 가설은 0을 예측하게 됩니다. 즉, 아직 적절한 W와 b의 값이 아닙니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBJ_hijdAddN"
   },
   "source": [
    "파이토치 코드 상으로 직선의 방정식에 해당되는 가설을 선언합니다.\n",
    "\n",
    "$ H(x)= Wx+b$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "svSbMlUiYbcz"
   },
   "outputs": [],
   "source": [
    "hypothesis = x_train * W + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjB6WdL9AfHK"
   },
   "source": [
    "파이토치 코드 상으로 선형 회귀의 비용 함수에 해당되는 평균 제곱 오차를 선언합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrU-YYjjAlh4"
   },
   "source": [
    "$$\n",
    "cost(W, b) = \\frac{1}{n} \\sum_{i=1}^{n} \\left[y^{(i)} - H(x^{(i)})\\right]^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cePGtsoMYfXt",
    "outputId": "a9d18310-ce4a-45c8-dc23-9d7997575eb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 앞서 배운 torch.mean으로 평균을 구한다.\n",
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VruMrO5UAqBG"
   },
   "source": [
    "이제 경사 하강법을 구현합니다. 아래의 'SGD'는 경사 하강법의 일종입니다. lr은 학습률(learning rate)를 의미합니다.\n",
    "학습 대상인 W와 b가 SGD의 입력이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VjrNx6RJYiEU"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([W, b], lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbgYlJwqArnY"
   },
   "source": [
    "optimizer.zero_grad()를 실행하므로서 미분을 통해 얻은 기울기를 0으로 초기화합니다. 기울기를 초기화해야만 새로운 가중치 편향에 대해서 새로운 기울기를 구할 수 있습니다.  \n",
    "\n",
    "그 다음 cost.backward() 함수를 호출하면 가중치 W와 편향 b에 대한 기울기가 계산됩니다.  \n",
    "\n",
    "그 다음 경사 하강법 최적화 함수 opimizer의 .step() 함수를 호출하여 인수로 들어갔던 W와 b에서 리턴되는 변수들의 기울기에 학습률(learining rate) 0.01을 곱하여 빼줌으로서 업데이트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "id": "aeQ658JrYnnb",
    "outputId": "bf017d48-8b5d-416a-be9e-0e250582bcf0"
   },
   "outputs": [],
   "source": [
    "# gradient를 0으로 초기화\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 비용 함수를 미분하여 gradient 계산\n",
    "cost.backward()\n",
    "\n",
    "# W와 b를 업데이트\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mDsAkngAuhk"
   },
   "source": [
    "결과적으로 훈련 과정에서 $W$와 $b$는 훈련 데이터와 잘 맞는 직선을 표현하기 위한 적절한 값으로 변화해갑니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3qd-nPMlYrOP",
    "outputId": "eb33e409-f953-4a92-e219-6091f03bc5bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  100/2000 W: 1.745, b: 0.579 Cost: 0.048403\n",
      "Epoch  200/2000 W: 1.800, b: 0.456 Cost: 0.029910\n",
      "Epoch  300/2000 W: 1.842, b: 0.358 Cost: 0.018483\n",
      "Epoch  400/2000 W: 1.876, b: 0.281 Cost: 0.011421\n",
      "Epoch  500/2000 W: 1.903, b: 0.221 Cost: 0.007058\n",
      "Epoch  600/2000 W: 1.923, b: 0.174 Cost: 0.004361\n",
      "Epoch  700/2000 W: 1.940, b: 0.137 Cost: 0.002695\n",
      "Epoch  800/2000 W: 1.953, b: 0.107 Cost: 0.001665\n",
      "Epoch  900/2000 W: 1.963, b: 0.084 Cost: 0.001029\n",
      "Epoch 1000/2000 W: 1.971, b: 0.066 Cost: 0.000636\n",
      "Epoch 1100/2000 W: 1.977, b: 0.052 Cost: 0.000393\n",
      "Epoch 1200/2000 W: 1.982, b: 0.041 Cost: 0.000243\n",
      "Epoch 1300/2000 W: 1.986, b: 0.032 Cost: 0.000150\n",
      "Epoch 1400/2000 W: 1.989, b: 0.025 Cost: 0.000093\n",
      "Epoch 1500/2000 W: 1.991, b: 0.020 Cost: 0.000057\n",
      "Epoch 1600/2000 W: 1.993, b: 0.016 Cost: 0.000035\n",
      "Epoch 1700/2000 W: 1.995, b: 0.012 Cost: 0.000022\n",
      "Epoch 1800/2000 W: 1.996, b: 0.010 Cost: 0.000014\n",
      "Epoch 1900/2000 W: 1.997, b: 0.008 Cost: 0.000008\n",
      "Epoch 2000/2000 W: 1.997, b: 0.006 Cost: 0.000005\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 2000 # 원하는만큼 경사 하강법을 반복\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCtx5zGB-3Ug"
   },
   "source": [
    "에포크(Epoch)는 전체 훈련 데이터가 학습에 한 번 사용된 주기를 말합니다.\n",
    "이번 실습의 경우 2,000번을 수행했습니다.\n",
    "\n",
    "최종 훈련 결과를 보면 최적의 기울기 W는 2에 가깝고, b는 0에 가까운 것을 볼 수 있습니다. 현재 훈련 데이터가 x_train은 [[1], [2], [3]]이고 y_train은 [[2], [4], [6]]인 것을 감안하면 실제 정답은 W가 2이고, b가 0이므로 거의 정답을 찾은 셈입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrQ1rAuLxmz2"
   },
   "source": [
    "# 2. torch.manual_seed()를 하는 이유"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grSMas30_Bli"
   },
   "source": [
    "torch.manual_seed()를 사용한 프로그램의 결과는 다른 컴퓨터에서 실행시켜도 동일한 결과를 얻을 수 있습니다.  \n",
    "\n",
    "그 이유는 torch.manual_seed()는 난수 발생 순서와 값을 동일하게 보장해준다는 특징때문입니다. 우선 랜덤 시드가 3일 때 두 번 난수를 발생시켜보고, 다른 랜덤 시드를 사용한 후에 다시 랜덤 시드를 3을 사용한다면 난수 발생값이 동일하게 나오는지 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6-3BW1tkxtKP"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vjq69tSaxt-n",
    "outputId": "2c1f8c3a-ee14-47d4-e9dd-a2543963bc20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시드가 3일 때\n",
      "tensor([0.0043])\n",
      "tensor([0.1056])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "print('랜덤 시드가 3일 때')\n",
    "for i in range(1,3):\n",
    "  print(torch.rand(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDoWgc_h_DaS"
   },
   "source": [
    "랜덤 시드가 3일때 두 개의 난수를 발생시켰더니 0.0043과 0.1056이 나옵니다. 이제 랜덤 시드값을 바꿔봅시다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXOOxrUSxu_u",
    "outputId": "088b6778-3c18-467f-e06d-e45d6c239cfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시드가 5일 때\n",
      "tensor([0.8303])\n",
      "tensor([0.1261])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "print('랜덤 시드가 5일 때')\n",
    "for i in range(1,3):\n",
    "  print(torch.rand(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnxNG7la_FL0"
   },
   "source": [
    "0.8303과 0.1261이 나옵니다. 이제 다시 랜덤 시드값을 3으로 돌려보겠습니다. 이렇게 하면 프로그램을 다시 처음부터 실행한 것처럼 난수 발생 순서가 초기화됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJ5F-olVxxwO",
    "outputId": "a3964a4b-f850-45c9-828e-650826fe89fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시드가 3일 때\n",
      "tensor([0.0043])\n",
      "tensor([0.1056])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "print('랜덤 시드가 3일 때')\n",
    "for i in range(1,3):\n",
    "  print(torch.rand(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aN0aBm7_G0S"
   },
   "source": [
    "다시 동일하게 0.0043과 0.1056이 나옵니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uv22uZ_jxz_b"
   },
   "source": [
    "# 3. optimizer.zero_grad()가 필요한 이유"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPD_BT-l-tWy"
   },
   "source": [
    "파이토치는 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값에 누적시키는 특징이 있습니다. 예를 들어봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gPEsldbqxyp2",
    "outputId": "f7d3dbc0-5c8b-4f48-9c3c-69e83781d59d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w로 미분한 값 : 2.0\n",
      "수식을 w로 미분한 값 : 4.0\n",
      "수식을 w로 미분한 값 : 6.0\n",
      "수식을 w로 미분한 값 : 8.0\n",
      "수식을 w로 미분한 값 : 10.0\n",
      "수식을 w로 미분한 값 : 12.0\n",
      "수식을 w로 미분한 값 : 14.0\n",
      "수식을 w로 미분한 값 : 16.0\n",
      "수식을 w로 미분한 값 : 18.0\n",
      "수식을 w로 미분한 값 : 20.0\n",
      "수식을 w로 미분한 값 : 22.0\n",
      "수식을 w로 미분한 값 : 24.0\n",
      "수식을 w로 미분한 값 : 26.0\n",
      "수식을 w로 미분한 값 : 28.0\n",
      "수식을 w로 미분한 값 : 30.0\n",
      "수식을 w로 미분한 값 : 32.0\n",
      "수식을 w로 미분한 값 : 34.0\n",
      "수식을 w로 미분한 값 : 36.0\n",
      "수식을 w로 미분한 값 : 38.0\n",
      "수식을 w로 미분한 값 : 40.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "\n",
    "  z = 2 * w\n",
    "\n",
    "  z.backward()\n",
    "  print('수식을 w로 미분한 값 : {}'.format(w.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cp_-ZC47-0vp"
   },
   "source": [
    "계속해서 미분값인 2가 누적되는 것을 볼 수 있습니다. 그렇기 때문에 optimizer.zero_grad()를 통해 미분값을 계속 0으로 초기화시켜줘야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTTmw5ZoOjgi"
   },
   "source": [
    "# 4. 로지스틱 회귀 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9I322nuCNxll"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "coJCmQfrN3kl",
    "outputId": "23a9635e-a8c2-43a9-f60c-c57b84c5266e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1c9dbc4930>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlDB31BbBdQs"
   },
   "source": [
    "x_train과 y_train을 텐서로 선언합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "11U-YGL4N5Nx"
   },
   "outputs": [],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZEsgQycBbv8"
   },
   "source": [
    "앞서 훈련 데이터를 행렬로 선언하고, 행렬 연산으로 가설을 세우는 방법을 배웠습니다.\n",
    "여기서도 마찬가지로 행렬 연산을 사용하여 가설식을 세울겁니다. x_train과 y_train의 크기를 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "su-HBNHSN8q8",
    "outputId": "b2e65535-7745-4d73-bd62-f5993ed1e29b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2])\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPtehhzdBhbs"
   },
   "source": [
    "현재 x_train은 6 × 2의 크기(shape)를 가지는 행렬이며, y_train은 6 × 1의 크기를 가지는 벡터입니다. x_train을 $X$라고 하고, 이와 곱해지는 가중치 벡터를 $W$라고 하였을 때, $XW$가 성립되기 위해서는 $W$ 벡터의 크기는 2 × 1이어야 합니다. 이제 W와 b를 선언합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3Y-kujiN989"
   },
   "outputs": [],
   "source": [
    "W = torch.zeros((2, 1), requires_grad=True) # 크기는 2 x 1\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szgKEW8HBjpT"
   },
   "source": [
    "이제 가설식을 세워보겠습니다. 파이토치에서는 $e^{x}$를 구현하기 위해서 torch.exp(x)를 사용합니다.  \n",
    "이에 따라 행렬 연산을 사용한 가설식은 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2zJ8cQEN_9c"
   },
   "outputs": [],
   "source": [
    "hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y01WKTpdBlh6"
   },
   "source": [
    "앞서 W와 b는 torch.zeros를 통해 전부 0으로 초기화 된 상태입니다. 이 상태에서 예측값을 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHLJgC-IOBfj",
    "outputId": "0ab9a175-cf4d-412b-82a4-6c783e359844"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis) # 예측값인 H(x) 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itfS-S-JBo2M"
   },
   "source": [
    "실제값 y_train과 크기가 동일한 6 × 1의 크기를 가지는 예측값 벡터가 나오는데 모든 값이 0.5입니다.  \n",
    "\n",
    "사실 가설식을 좀 더 간단하게도 구현할 수 있습니다. 이미 파이토치에서는 시그모이드 함수를 이미 구현하여 제공하고 있기 때문입니다. 다음은 torch.sigmoid를 사용하여 좀 더 간단히 구현한 가설식입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XkFu19kOC3E"
   },
   "outputs": [],
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeFkVy-LBqXg"
   },
   "source": [
    "앞서 구현한 식과 본질적으로 동일한 식입니다. 마찬가지로 W와 b가 0으로 초기화 된 상태에서 예측값을 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B89fHH4vOEJf",
    "outputId": "f503d5a1-462c-4d5d-e496-bfdadbef8b7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1ot_G0MCA9j"
   },
   "source": [
    "앞선 결과와 동일하게 y_train과 크기가 동일한 6 × 1의 크기를 가지는 예측값 벡터가 나오는데 모든 값이 0.5입니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kb_89q-4CHwn"
   },
   "source": [
    "이제 아래의 비용 함수값. 즉, 현재 예측값과 실제값 사이의 cost를 구해보겠습니다.\n",
    "$$\n",
    "cost(W) = -\\frac{1}{n} \\sum_{i=1}^{n} [y^{(i)}logH(x^{(i)}) + (1-y^{(i)})log(1-H(x^{(i)}))]\n",
    "$$\n",
    "우선, 현재 예측값과 실제값을 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bnbag6_pOFtZ",
    "outputId": "1c5348c2-6daf-49e7-8f65-f125476cb5a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2fM-faZCNI7"
   },
   "source": [
    "현재 총 6개의 원소가 존재하지만 하나의 샘플. 즉, 하나의 원소에 대해서만 오차를 구하는 식을 작성해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJp83njpOG_F",
    "outputId": "64957eb3-e2cd-4af6-8c55-bed8fcfe498f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6931], grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(y_train[0] *torch.log(hypothesis[0]) +\n",
    "  (1-y_train[0]) *torch.log(1-hypothesis[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gjjtTJACO0o"
   },
   "source": [
    "이제 모든 원소에 대해서 오차를 구해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LuEOT6IaOIfg",
    "outputId": "d2a40afb-deb6-47bb-8f1a-741f962bcb6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931]], grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "losses = -(y_train * torch.log(hypothesis) +\n",
    "           (1 - y_train) * torch.log(1 - hypothesis))\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tY_WBJaCCQDu"
   },
   "source": [
    "그리고 이 전체 오차에 대한 평균을 구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Khm12-7hOJ_Y",
    "outputId": "931976e5-ad1b-47e7-d5fc-4e299256598f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = losses.mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YW5KpgyACSjF"
   },
   "source": [
    "결과적으로 얻은 cost는 0.6931입니다.  \n",
    "\n",
    "지금까지 비용 함수의 값을 직접 구현하였는데, 사실 파이토치에서는 로지스틱 회귀의 비용 함수를 이미 구현해서 제공하고 있습니다.  \n",
    "사용 방법은 torch.nn.functional as F와 같이 임포트 한 후에 F.binary_cross_entropy(예측값, 실제값)과 같이 사용하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_opkTIzEOL_n",
    "outputId": "556db183-bfeb-44e1-f5c7-da9808ae2e1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy(hypothesis, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsYmUb4xCU-l"
   },
   "source": [
    "동일하게 cost가 0.6931이 출력되는 것을 볼 수 있습니다. 모델의 훈련 과정까지 추가한 전체 코드는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWwB32BJONbv"
   },
   "outputs": [],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z37fgZ-mOPIy",
    "outputId": "37e225dd-6a5d-4eeb-b502-1d391b9d08f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  100/1000 Cost: 0.135644\n",
      "Epoch  200/1000 Cost: 0.080964\n",
      "Epoch  300/1000 Cost: 0.058062\n",
      "Epoch  400/1000 Cost: 0.045398\n",
      "Epoch  500/1000 Cost: 0.037327\n",
      "Epoch  600/1000 Cost: 0.031720\n",
      "Epoch  700/1000 Cost: 0.027592\n",
      "Epoch  800/1000 Cost: 0.024422\n",
      "Epoch  900/1000 Cost: 0.021911\n",
      "Epoch 1000/1000 Cost: 0.019871\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((2, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    # Cost 계산\n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "    cost = -(y_train * torch.log(hypothesis) +\n",
    "             (1 - y_train) * torch.log(1 - hypothesis)).mean()\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JM0x5T2fCXEf"
   },
   "source": [
    "학습이 끝났습니다. 이제 훈련했던 훈련 데이터를 그대로 입력으로 사용했을 때, 제대로 예측하는지 확인해보겠습니다.  \n",
    "현재 W와 b는 훈련 후의 값을 가지고 있습니다. 현재 W와 b를 가지고 예측값을 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VWssyJHFOQ_r",
    "outputId": "b0324523-a9a1-4793-8a50-becee7634660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.7711e-04],\n",
      "        [3.1636e-02],\n",
      "        [3.9014e-02],\n",
      "        [9.5618e-01],\n",
      "        [9.9823e-01],\n",
      "        [9.9969e-01]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8K5KvC8CaNX"
   },
   "source": [
    "현재 위 값들은 0과 1 사이의 값을 가지고 있습니다. 이제 0.5를 넘으면 True, 넘지 않으면 False로 값을 정하여 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tb2WN7pcOYOg",
    "outputId": "efd60ec3-9b58-4e63-cce9-8faabb6149d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True]])\n"
     ]
    }
   ],
   "source": [
    "prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4MWMxZJCbm1"
   },
   "source": [
    "실제값은 [[0], [0], [0], [1], [1], [1]]이므로, 이는 결과적으로 False, False, False, True, True, True와 동일합니다. 즉, 기존의 실제값과 동일하게 예측한 것을 볼 수 있습니다. 훈련이 된 후의 W와 b의 값을 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IFVnA3gwOZpE",
    "outputId": "e6f7ccfa-887d-40b4-c93c-c4ba7048f558"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.2521],\n",
      "        [1.5174]], requires_grad=True)\n",
      "tensor([-14.4777], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. `nn.Linear` <br>\n",
    "\n",
    "- `nn.Linear`는 PyTorch에서 __선형 변환(Linear Transformation)__ 을 구현하는 모듈로, 선형 회귀의 핵심인 \\( y = Wx + b \\)를 표현할 수 있습니다.  \n",
    "하지만, `nn.Linear`는 단순히 선형 변환을 수행하는 도구일 뿐, **손실 함수**나 **최적화 알고리즘**과 결합되어야 선형 회귀 모델로 완성됩니다.<br>\n",
    "여러가지 이름으로 사용되기는 하지만 fc layer(Fully Connected layer) 라고 표현합니다.\n",
    "\n",
    "---\n",
    "\n",
    "### [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)의 구성 요소\n",
    "\n",
    "`nn.Linear`는 다음 두 가지 학습 가능한 매개변수를 포함합니다:\n",
    "1. **가중치 \\( W \\)**: 입력과 출력 간의 관계를 나타내는 행렬.\n",
    "2. **편향 \\( b \\)**: 출력값에 추가되는 상수.\n",
    "\n",
    "이 두 매개변수는 **경사하강법(Gradient Descent)**을 통해 학습됩니다.\n",
    "---\n",
    "\n",
    "### `nn.Linear`와 선형 회귀의 차이점\n",
    "\n",
    "- **선형 회귀**:\n",
    "  - \\( y = Wx + b \\)를 사용하여 입력 \\( x \\)와 출력 \\( y \\) 사이의 선형 관계를 학습합니다.\n",
    "  - 손실 함수로 주로 MSE(평균 제곱 오차)를 사용합니다.\n",
    "  - 학습 알고리즘(예: 경사하강법)을 통해 최적화됩니다.\n",
    "\n",
    "- **`nn.Linear`**:\n",
    "  - 입력과 출력 간의 선형 변환을 계산하는 모듈입니다.\n",
    "  - 학습 과정에서 손실 함수와 옵티마이저를 따로 정의해야 합니다.\n",
    "  - 선형 회귀뿐만 아니라 다른 모델(예: 신경망)에서도 사용됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape : torch.Size([3, 1])\n",
      "\n",
      "FC layer : Linear(in_features=1, out_features=1, bias=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "x_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "\n",
    "print(f\"x shape : {x_train.shape}\") # 1차원 짜리 Vector가 3개가 있고 1개의 Feature가진다\n",
    "print()\n",
    "fc = nn.Linear(1, 1) # input feature, output feature \n",
    "\n",
    "\n",
    "print(f\"FC layer : {fc}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight : Parameter containing:\n",
      "tensor([[0.5153]], requires_grad=True)\n",
      "Bias : Parameter containing:\n",
      "tensor([-0.4414], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Weight : {fc.weight}\")\n",
    "print(f\"Bias : {fc.bias}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight : Parameter containing:\n",
      "tensor([[0.]], requires_grad=True)\n",
      "Bias : Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# fc layer의 weight와 bias를 초기화 할 수 있습니다.  \n",
    "fc.weight.data.fill_(0)\n",
    "fc.bias.data.fill_(0)\n",
    "\n",
    "print(f\"Weight : {fc.weight}\")\n",
    "print(f\"Bias : {fc.bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "h = fc(x_train)\n",
    "print(h) # grad_fn=<AddmmBackward0> autograd 연산 속성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "직접 구현한 선형 회귀:\n",
      "Epoch  200/1000 W: 1.800, b: 0.456, Cost: 0.029910\n",
      "Epoch  400/1000 W: 1.876, b: 0.281, Cost: 0.011421\n",
      "Epoch  600/1000 W: 1.923, b: 0.174, Cost: 0.004361\n",
      "Epoch  800/1000 W: 1.953, b: 0.107, Cost: 0.001665\n",
      "Epoch 1000/1000 W: 1.971, b: 0.066, Cost: 0.000636\n",
      "\n",
      "`nn.Linear`를 사용한 선형 회귀:\n",
      "Epoch  200/1000 W: 1.800, b: 0.456, Cost: 0.029910\n",
      "Epoch  400/1000 W: 1.876, b: 0.281, Cost: 0.011421\n",
      "Epoch  600/1000 W: 1.923, b: 0.174, Cost: 0.004361\n",
      "Epoch  800/1000 W: 1.953, b: 0.107, Cost: 0.001665\n",
      "Epoch 1000/1000 W: 1.971, b: 0.066, Cost: 0.000636\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 데이터 준비\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "# 1. 직접 구현한 선형 회귀\n",
    "# 가중치(W)와 편향(b) 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# 학습 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "epochs = 1000\n",
    "\n",
    "# 학습 루프\n",
    "print(\"직접 구현한 선형 회귀:\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # 예측값 계산\n",
    "    hypothesis = x_train * W + b\n",
    "    \n",
    "    # 손실 계산 (MSE)\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    # 경사 초기화, 역전파, 매개변수 업데이트\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 로그 출력\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch:4d}/{epochs} W: {W.item():.3f}, b: {b.item():.3f}, Cost: {cost.item():.6f}\")\n",
    "\n",
    "# 2. nn.Linear를 사용한 선형 회귀\n",
    "# 모델 정의\n",
    "fc = nn.Linear(1, 1)  # 입력 크기: 1, 출력 크기: 1\n",
    "optimizer = optim.SGD(fc.parameters(), lr=0.01)\n",
    "fc.weight.data.fill_(0)\n",
    "fc.bias.data.fill_(0)\n",
    "# 학습 루프\n",
    "print(\"\\n`nn.Linear`를 사용한 선형 회귀:\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # 예측값 계산\n",
    "    prediction = fc(x_train)\n",
    "    \n",
    "    # 손실 계산 (MSE)\n",
    "    cost = nn.MSELoss()(prediction, y_train)\n",
    "    \n",
    "    # 경사 초기화, 역전파, 매개변수 업데이트\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 로그 출력\n",
    "    if epoch % 200 == 0:\n",
    "        W, b = fc.parameters()\n",
    "        print(f\"Epoch {epoch:4d}/{epochs} W: {W.item():.3f}, b: {b.item():.3f}, Cost: {cost.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 소프트맥스 함수 직접 구현\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 소프트맥스 함수란?\n",
    "\n",
    "소프트맥스(Softmax) 함수는 **다중 클래스 분류** 문제에서 사용되며, 모델이 각 클래스에 대해 예측한 점수(logits)를 **확률 분포**로 변환합니다.  \n",
    "입력 값의 상대적 크기를 기반으로 모든 클래스의 확률 합이 \\(1\\)이 되도록 정규화합니다.\n",
    "\n",
    "### 소프트맥스 함수 공식:\n",
    "\n",
    "## $$ \\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j}} $$\n",
    "\n",
    "- $(z_i)$: 클래스 $(i)$의 점수(logit).\n",
    "- $(e^{z_i})$: 클래스 $(i)$의 점수를 지수 함수로 변환.\n",
    "- $(\\sum_{j=1}^n e^{z_j})$: 모든 클래스의 지수 값의 합.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 소프트맥스 함수의 특징\n",
    "\n",
    "1. **출력 값의 범위**:\n",
    "   $\n",
    "   0 \\leq \\text{Softmax}(z_i) \\leq 1\n",
    "   $\n",
    "   - 모든 출력 값은 확률로 해석 가능.\n",
    "\n",
    "2. **확률 분포**:\n",
    "   $\n",
    "   \\sum_{i=1}^n \\text{Softmax}(z_i) = 1\n",
    "   $\n",
    "   - 출력 값의 합이 항상 1이 됩니다.\n",
    "\n",
    "3. **다중 클래스 분류**:\n",
    "   - 소프트맥스는 각 클래스에 대한 **확률값**을 제공하여 가장 가능성이 높은 클래스를 예측할 수 있도록 합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 소프트맥스 함수 직접 구현\n",
    "\n",
    "소프트맥스 함수는 두 단계로 구현됩니다:\n",
    "1. 각 점수(logit)를 지수 함수로 변환: $(e^{z_i})$.\n",
    "2. 모든 지수 값의 합으로 각 값을 나누어 정규화.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost (Builtin): 1.333151 Cost (Manual): 1.333150\n",
      "Epoch  100/1000 Cost (Builtin): 0.579484 Cost (Manual): 0.579484\n",
      "Epoch  200/1000 Cost (Builtin): 0.518470 Cost (Manual): 0.518471\n",
      "Epoch  300/1000 Cost (Builtin): 0.487488 Cost (Manual): 0.487488\n",
      "Epoch  400/1000 Cost (Builtin): 0.467566 Cost (Manual): 0.467566\n",
      "Epoch  500/1000 Cost (Builtin): 0.453135 Cost (Manual): 0.453135\n",
      "Epoch  600/1000 Cost (Builtin): 0.441908 Cost (Manual): 0.441908\n",
      "Epoch  700/1000 Cost (Builtin): 0.432757 Cost (Manual): 0.432757\n",
      "Epoch  800/1000 Cost (Builtin): 0.425052 Cost (Manual): 0.425052\n",
      "Epoch  900/1000 Cost (Builtin): 0.418410 Cost (Manual): 0.418410\n",
      "Epoch 1000/1000 Cost (Builtin): 0.412582 Cost (Manual): 0.412582\n",
      "\n",
      "Test Data: tensor([[4., 2.]])\n",
      "Logits (z): tensor([[0.8664, 0.4130, 1.2396]])\n",
      "Softmax Output (Manual): tensor([[0.3239, 0.2058, 0.4703]])\n",
      "Predictions (y_pred): tensor([2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. 데이터 준비\n",
    "x_train = torch.FloatTensor([[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]])\n",
    "y_train = torch.LongTensor([0, 1, 2, 0, 1, 2])  # 클래스 레이블 (0, 1, 2)\n",
    "\n",
    "# 2. 모델 정의\n",
    "class SoftmaxClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SoftmaxClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(2, 3)  # 입력 크기 2, 출력 크기 3 (클래스 개수)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)  # 소프트맥스는 CrossEntropyLoss에서 자동 계산\n",
    "\n",
    "model = SoftmaxClassifier()\n",
    "\n",
    "# 3. 손실 함수와 옵티마이저 정의\n",
    "# PyTorch 내장 크로스 엔트로피\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 4. 직접 구현한 크로스 엔트로피 손실 함수\n",
    "def cross_entropy_loss(z, y):\n",
    "    # 1. 소프트맥스 계산\n",
    "    softmax = torch.exp(z) / torch.sum(torch.exp(z), dim=1, keepdim=True)\n",
    "    # 2. 정답 클래스의 확률 추출\n",
    "    log_probs = torch.log(softmax)\n",
    "    target_log_probs = log_probs[range(len(y)), y]\n",
    "    # 3. 손실 계산 (음수 로그 확률 평균)\n",
    "    cost = -torch.mean(target_log_probs)\n",
    "    return cost\n",
    "\n",
    "# 5. 훈련 과정\n",
    "epochs = 1000\n",
    "for epoch in range(epochs + 1):\n",
    "    # Forward\n",
    "    z = model(x_train)  # 모델의 예측값 (logits)\n",
    "    \n",
    "    # PyTorch 내장 손실\n",
    "    cost_builtin = criterion(z, y_train)\n",
    "    \n",
    "    # 직접 구현한 손실\n",
    "    cost_manual = cross_entropy_loss(z, y_train)\n",
    "    \n",
    "    # Backward (직접 구현한 손실로 학습 진행)\n",
    "    optimizer.zero_grad()\n",
    "    cost_manual.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch:4d}/{epochs} Cost (Builtin): {cost_builtin.item():.6f} Cost (Manual): {cost_manual.item():.6f}')\n",
    "\n",
    "# 6. 모델 평가 (직접 구현한 소프트맥스 사용)\n",
    "with torch.no_grad():\n",
    "    x_test = torch.FloatTensor([[4, 2]])  # 예측을 위한 새로운 데이터\n",
    "    z = model(x_test)\n",
    "    \n",
    "    # 소프트맥스 직접 구현\n",
    "    exp_z = torch.exp(z)  # 각 logit에 대해 e^z 계산\n",
    "    softmax_output = exp_z / torch.sum(exp_z, dim=1, keepdim=True)  # 확률 계산\n",
    "\n",
    "    y_pred = torch.argmax(softmax_output, dim=1)  # 가장 높은 확률의 클래스 선택\n",
    "\n",
    "    print(\"\\nTest Data:\", x_test)\n",
    "    print(\"Logits (z):\", z)\n",
    "    print(\"Softmax Output (Manual):\", softmax_output)\n",
    "    print(\"Predictions (y_pred):\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Data: tensor([[4., 2.]])\n",
      "Logits: tensor([[0.8664, 0.4130, 1.2396]])\n",
      "Softmax Output: tensor([[0.3239, 0.2058, 0.4703]])\n",
      "Predictions: tensor([2])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# 5. 모델 평가\n",
    "with torch.no_grad():\n",
    "    test_data = torch.FloatTensor([[4, 2]])  # 예측을 위한 새로운 데이터\n",
    "    logits = model(test_data)\n",
    "    softmax_output = torch.softmax(logits, dim=1)  # 소프트맥스 확률\n",
    "    predictions = torch.argmax(softmax_output, dim=1)  # 가장 높은 확률의 클래스 선택\n",
    "\n",
    "    print(\"\\nTest Data:\", test_data)\n",
    "    print(\"Logits:\", logits)\n",
    "    print(\"Softmax Output:\", softmax_output)\n",
    "    print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
